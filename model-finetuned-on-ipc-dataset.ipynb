{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TUSHAR NAGPAL -MT2022125\n## MANDATE 4 : Final Submission\n### This file contains code to fine-tune BERT on IPC_Dataset created using Haystack-annotation tool as mentioned in Mandate 2","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-04T07:11:05.903766Z","iopub.execute_input":"2023-05-04T07:11:05.904198Z","iopub.status.idle":"2023-05-04T07:11:05.925363Z","shell.execute_reply.started":"2023-05-04T07:11:05.904162Z","shell.execute_reply":"2023-05-04T07:11:05.922971Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/ipc-context/IPC_dataset.json\n/kaggle/input/ipc-context/file.txt\n/kaggle/input/ipc-context/ipc_haystack.json\n/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U nltk","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:05.928317Z","iopub.execute_input":"2023-05-04T07:11:05.929703Z","iopub.status.idle":"2023-05-04T07:11:15.688184Z","shell.execute_reply.started":"2023-05-04T07:11:05.929661Z","shell.execute_reply":"2023-05-04T07:11:15.686999Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.8.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.11.4)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.11.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nimport nltk\nimport urllib.request\nimport urllib\n!nltk.download('reuters')","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:15.690448Z","iopub.execute_input":"2023-05-04T07:11:15.690981Z","iopub.status.idle":"2023-05-04T07:11:17.239003Z","shell.execute_reply.started":"2023-05-04T07:11:15.690929Z","shell.execute_reply":"2023-05-04T07:11:17.237721Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/bin/bash: -c: line 0: syntax error near unexpected token `'reuters''\n/bin/bash: -c: line 0: `nltk.download('reuters')'\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:17.242107Z","iopub.execute_input":"2023-05-04T07:11:17.242865Z","iopub.status.idle":"2023-05-04T07:11:27.481587Z","shell.execute_reply.started":"2023-05-04T07:11:17.242816Z","shell.execute_reply":"2023-05-04T07:11:27.480298Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### This code is to read the json dataset and convert it in train-test split","metadata":{}},{"cell_type":"code","source":"def read_ipc(data_dict, split):\n    contexts = []\n    questions = []\n    answers = []\n    train_contexts = []\n    train_questions = []\n    train_answers = []\n    val_contexts = []\n    val_questions = []\n    val_answers = []\n    \n    for x,group in enumerate(data_dict['data']):\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    if answer['answer_start']!=-1:\n                        contexts.append(context)\n                        questions.append(question)\n                        answers.append(answer)\n    \n    ##### splitting wrt number split given#####\n    train_end=int(len(contexts)*split)\n    ## 0 to 294 \n    \n    for x in range(train_end):\n        train_contexts.append(contexts[x])\n        train_questions.append(questions[x])\n        train_answers.append(answers[x])\n    print(len(train_answers))\n    ### 295 to end of doc\n    for x in range(train_end,len(contexts)):\n        val_contexts.append(contexts[x])\n        val_questions.append(questions[x])\n        val_answers.append(answers[x])\n        \n    print(len(val_answers))   \n    return train_contexts, train_questions, train_answers,val_contexts, val_questions, val_answers","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.486444Z","iopub.execute_input":"2023-05-04T07:11:27.486884Z","iopub.status.idle":"2023-05-04T07:11:27.499254Z","shell.execute_reply.started":"2023-05-04T07:11:27.486836Z","shell.execute_reply":"2023-05-04T07:11:27.498100Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Loading the dataset for IPC which I have created using pdf available at https://www.indiacode.nic.in/\n#### Dataset is made by separating each Indian Penal Code Section.","metadata":{}},{"cell_type":"code","source":"f = open('/kaggle/input/ipc-context/ipc_haystack.json')\ndata = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.500316Z","iopub.execute_input":"2023-05-04T07:11:27.500912Z","iopub.status.idle":"2023-05-04T07:11:27.525187Z","shell.execute_reply.started":"2023-05-04T07:11:27.500865Z","shell.execute_reply":"2023-05-04T07:11:27.524108Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"contexts = pd.json_normalize(data)\ncon = data['data'][0]['paragraphs'][0]['context']\ncon\n\ncont = []\nfor d in data['data']:\n    for p in d['paragraphs']:\n        cont.append(p['context'])\ncont\ncontDF = pd.DataFrame(cont)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.535652Z","iopub.execute_input":"2023-05-04T07:11:27.537147Z","iopub.status.idle":"2023-05-04T07:11:27.547543Z","shell.execute_reply.started":"2023-05-04T07:11:27.537108Z","shell.execute_reply":"2023-05-04T07:11:27.546318Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"contDF.rename(columns = {0: 'description'})","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.549139Z","iopub.execute_input":"2023-05-04T07:11:27.550358Z","iopub.status.idle":"2023-05-04T07:11:27.573678Z","shell.execute_reply.started":"2023-05-04T07:11:27.550320Z","shell.execute_reply":"2023-05-04T07:11:27.572609Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                            description\n0             Title and extent of operation of the Code\n1     This Act shall be called the Indian Penal Code...\n2         Punishment of offences committed within India\n3     Every person shall be liable to punishment und...\n4     Punishment of offences committed beyond, but w...\n...                                                 ...\n1015  Whoever, intending to insult the modesty of an...\n1016           Misconduct in public by a drunken person\n1017  Whoever, in a state of intoxication, appears i...\n1018  Punishment for attempting to commit offences p...\n1019  Whoever attempts to commit an offence punishab...\n\n[1020 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Title and extent of operation of the Code</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This Act shall be called the Indian Penal Code...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Punishment of offences committed within India</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Every person shall be liable to punishment und...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Punishment of offences committed beyond, but w...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1015</th>\n      <td>Whoever, intending to insult the modesty of an...</td>\n    </tr>\n    <tr>\n      <th>1016</th>\n      <td>Misconduct in public by a drunken person</td>\n    </tr>\n    <tr>\n      <th>1017</th>\n      <td>Whoever, in a state of intoxication, appears i...</td>\n    </tr>\n    <tr>\n      <th>1018</th>\n      <td>Punishment for attempting to commit offences p...</td>\n    </tr>\n    <tr>\n      <th>1019</th>\n      <td>Whoever attempts to commit an offence punishab...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1020 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_contexts, train_questions, train_answers,val_contexts, val_questions, val_answers = read_ipc(data,0.8)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.575510Z","iopub.execute_input":"2023-05-04T07:11:27.576408Z","iopub.status.idle":"2023-05-04T07:11:27.584736Z","shell.execute_reply.started":"2023-05-04T07:11:27.576342Z","shell.execute_reply":"2023-05-04T07:11:27.583460Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"294\n74\n","output_type":"stream"}]},{"cell_type":"code","source":"def add_end_idx(answers, contexts):\n    for answer, context in zip(answers, contexts):\n        gold_text = answer['text']\n        start_idx = answer['answer_start']\n        end_idx = start_idx + len(gold_text)\n\n        if context[start_idx:end_idx] == gold_text:\n            answer['answer_end'] = end_idx\n        elif context[start_idx-1:end_idx-1] == gold_text:\n            answer['answer_start'] = start_idx - 1\n            answer['answer_end'] = end_idx - 1\n        elif context[start_idx-2:end_idx-2] == gold_text:\n            answer['answer_start'] = start_idx - 2\n            answer['answer_end'] = end_idx - 2\n\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(val_answers, val_contexts)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.590553Z","iopub.execute_input":"2023-05-04T07:11:27.591312Z","iopub.status.idle":"2023-05-04T07:11:27.600912Z","shell.execute_reply.started":"2023-05-04T07:11:27.591275Z","shell.execute_reply":"2023-05-04T07:11:27.599764Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:27.602493Z","iopub.execute_input":"2023-05-04T07:11:27.603275Z","iopub.status.idle":"2023-05-04T07:11:30.905665Z","shell.execute_reply.started":"2023-05-04T07:11:27.603237Z","shell.execute_reply":"2023-05-04T07:11:30.904466Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:30.907313Z","iopub.execute_input":"2023-05-04T07:11:30.907953Z","iopub.status.idle":"2023-05-04T07:11:30.915120Z","shell.execute_reply.started":"2023-05-04T07:11:30.907920Z","shell.execute_reply":"2023-05-04T07:11:30.913932Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n","output_type":"stream"}]},{"cell_type":"code","source":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    print(len(answers))\n    for i in range(len(answers)):\n        try:\n            start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n            # if None, the answer passage has been truncated\n            if start_positions[-1] is None:\n                start_positions[-1] = tokenizer.model_max_length\n            if end_positions[-1] is None:\n                end_positions[-1] = tokenizer.model_max_length\n        except:\n            print(answers[i],i)\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:30.917134Z","iopub.execute_input":"2023-05-04T07:11:30.918266Z","iopub.status.idle":"2023-05-04T07:11:30.932950Z","shell.execute_reply.started":"2023-05-04T07:11:30.918224Z","shell.execute_reply":"2023-05-04T07:11:30.931519Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"294\n74\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nclass IpcDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\ntrain_dataset = IpcDataset(train_encodings)\nval_dataset = IpcDataset(val_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:30.934263Z","iopub.execute_input":"2023-05-04T07:11:30.935277Z","iopub.status.idle":"2023-05-04T07:11:31.741396Z","shell.execute_reply.started":"2023-05-04T07:11:30.935237Z","shell.execute_reply":"2023-05-04T07:11:31.740299Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForQuestionAnswering\nmodel_db = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n# from transformers import BertForQuestionAnswering\n# model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:31.742964Z","iopub.execute_input":"2023-05-04T07:11:31.743329Z","iopub.status.idle":"2023-05-04T07:11:33.356061Z","shell.execute_reply.started":"2023-05-04T07:11:31.743288Z","shell.execute_reply":"2023-05-04T07:11:33.354858Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training Model","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel_db.to(device)\nmodel_db.train()\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\noptim = AdamW(model_db.parameters(), lr=5e-5)\n\nfor epoch in range(10):\n    for x,batch in enumerate(train_loader):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model_db(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n        if x == 10:\n            print(\"batch: \"+str(x)+\" loss: \"+str(loss))\nmodel_db.eval()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:11:33.357940Z","iopub.execute_input":"2023-05-04T07:11:33.358621Z","iopub.status.idle":"2023-05-04T07:14:10.846655Z","shell.execute_reply.started":"2023-05-04T07:11:33.358574Z","shell.execute_reply":"2023-05-04T07:14:10.845433Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"batch: 10 loss: tensor(2.1966, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(1.1751, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.9721, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.6014, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.7275, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.4809, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.3872, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.3263, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.2904, device='cuda:0', grad_fn=<DivBackward0>)\nbatch: 10 loss: tensor(0.2639, device='cuda:0', grad_fn=<DivBackward0>)\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DistilBertForQuestionAnswering(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### This question will be provided by user,\n### and paragraph will be predicted by model trained in file \"Predicting the context.ipynb\"","metadata":{}},{"cell_type":"code","source":"question = \"punishment if woman dies within seven years of marriage\"\n# paragraph=\"Whoever makes any preparation for committing dacoity, shall be punished with rigorous imprisonment for a term which may extend to ten years, and shall also be liable to fine.\"\n# val_contexts[2]\nparagraph = \"Injuring or defiling place of worship, with intent to insult the religion of any class. Whoever destroys, damages or defiles any place of worship, or any object held sacred by any class of persons with the intention of thereby insulting the religion of any class of persons or with the knowledge that any class of persons is likely to consider such destruction, damage or defilement as an insult to their religion, shall be punished with imprisonment of either description for a term which may extend to two years, or with fine, or with both.\"\n# paragraph = \"Word, gesture or act intended to insult the modesty of a woman.—Whoever, intending to insult the modesty of any woman, utters any words, makes any sound or gesture, or exhibits any object, intending that such word or sound shall be heard, or that such gesture or object shall be seen, by such woman, or intrudes upon the privacy of such woman, 1[shall be punished with simple imprisonment for a term which may extend to three years, and also with fine].\"\n# ans=val_answers[2]\n# val_answers[2]\nprint(question)\nprint(paragraph)\n# print(ans)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:22:23.343549Z","iopub.execute_input":"2023-05-04T07:22:23.344553Z","iopub.status.idle":"2023-05-04T07:22:23.352575Z","shell.execute_reply.started":"2023-05-04T07:22:23.344511Z","shell.execute_reply":"2023-05-04T07:22:23.351184Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"What is the punishment for defilement of place of worship\nInjuring or defiling place of worship, with intent to insult the religion of any class. Whoever destroys, damages or defiles any place of worship, or any object held sacred by any class of persons with the intention of thereby insulting the religion of any class of persons or with the knowledge that any class of persons is likely to consider such destruction, damage or defilement as an insult to their religion, shall be punished with imprisonment of either description for a term which may extend to two years, or with fine, or with both.\n","output_type":"stream"}]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(text=question, text_pair = paragraph)\ninput_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\ntokens = tokenizer.convert_ids_to_tokens(input_ids) #input tokens\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:22:24.870169Z","iopub.execute_input":"2023-05-04T07:22:24.870786Z","iopub.status.idle":"2023-05-04T07:22:24.878247Z","shell.execute_reply.started":"2023-05-04T07:22:24.870747Z","shell.execute_reply":"2023-05-04T07:22:24.876870Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"['[CLS]', 'what', 'is', 'the', 'punishment', 'for', 'def', '##ile', '##ment', 'of', 'place', 'of', 'worship', '[SEP]', 'injuring', 'or', 'def', '##iling', 'place', 'of', 'worship', ',', 'with', 'intent', 'to', 'insult', 'the', 'religion', 'of', 'any', 'class', '.', 'whoever', 'destroys', ',', 'damages', 'or', 'def', '##ile', '##s', 'any', 'place', 'of', 'worship', ',', 'or', 'any', 'object', 'held', 'sacred', 'by', 'any', 'class', 'of', 'persons', 'with', 'the', 'intention', 'of', 'thereby', 'insulting', 'the', 'religion', 'of', 'any', 'class', 'of', 'persons', 'or', 'with', 'the', 'knowledge', 'that', 'any', 'class', 'of', 'persons', 'is', 'likely', 'to', 'consider', 'such', 'destruction', ',', 'damage', 'or', 'def', '##ile', '##ment', 'as', 'an', 'insult', 'to', 'their', 'religion', ',', 'shall', 'be', 'punished', 'with', 'imprisonment', 'of', 'either', 'description', 'for', 'a', 'term', 'which', 'may', 'extend', 'to', 'two', 'years', ',', 'or', 'with', 'fine', ',', 'or', 'with', 'both', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"ip=torch.tensor([input_ids]).to(device)\nattention=torch.tensor([attention_mask]).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:22:25.468518Z","iopub.execute_input":"2023-05-04T07:22:25.469704Z","iopub.status.idle":"2023-05-04T07:22:25.476085Z","shell.execute_reply.started":"2023-05-04T07:22:25.469654Z","shell.execute_reply":"2023-05-04T07:22:25.474833Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Prediction:","metadata":{}},{"cell_type":"code","source":"output = model_db(ip, attention)\nstart_scores = output.start_logits\nend_scores = output.end_logits","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:22:25.649369Z","iopub.execute_input":"2023-05-04T07:22:25.649699Z","iopub.status.idle":"2023-05-04T07:22:25.672107Z","shell.execute_reply.started":"2023-05-04T07:22:25.649670Z","shell.execute_reply":"2023-05-04T07:22:25.671169Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"max_startscore = torch.argmax(start_scores)\nmax_endscore = torch.argmax(end_scores)\nans_tokens = input_ids[max_startscore: max_endscore + 1]\n# answer = ' '.join(tokens[start_index:end_index+1])\nprint(ans_tokens)\nanswer = ' '.join(tokens[max_startscore:max_endscore+1])\nprint(\"question: \"+str(question))\nprint(\"answer: \"+str(answer))\nprint(\"context: \"+str(paragraph))","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:22:26.098535Z","iopub.execute_input":"2023-05-04T07:22:26.102966Z","iopub.status.idle":"2023-05-04T07:22:26.111215Z","shell.execute_reply.started":"2023-05-04T07:22:26.102927Z","shell.execute_reply":"2023-05-04T07:22:26.109892Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[10219, 1997, 2593, 6412, 2005, 1037, 2744, 2029, 2089, 7949, 2000, 2048, 2086, 1010, 2030, 2007, 2986, 1010, 2030, 2007, 2119, 1012]\nquestion: What is the punishment for defilement of place of worship\nanswer: imprisonment of either description for a term which may extend to two years , or with fine , or with both .\ncontext: Injuring or defiling place of worship, with intent to insult the religion of any class. Whoever destroys, damages or defiles any place of worship, or any object held sacred by any class of persons with the intention of thereby insulting the religion of any class of persons or with the knowledge that any class of persons is likely to consider such destruction, damage or defilement as an insult to their religion, shall be punished with imprisonment of either description for a term which may extend to two years, or with fine, or with both.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Since, this model will just provide us answer of any question that user will ask related to IPC: Accuracy is not measure because the answers will be general.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}